<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Práctica Fine-Tuning Gemma 3 270M (MLX)</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background-color: #f0f2f5;
            color: #333;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
        }
        .container {
            max-width: 900px;
            margin: 0 auto;
            background-color: #fff;
            padding: 40px;
            border-radius: 10px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }
        h1 {
            text-align: center;
            color: #2c3e50;
            border-bottom: 2px solid #3498db;
            padding-bottom: 20px;
            margin-bottom: 40px;
        }
        .step {
            margin-bottom: 60px;
            border-bottom: 1px solid #eee;
            padding-bottom: 40px;
        }
        .step:last-child {
            border-bottom: none;
        }
        .explanation {
            background-color: #e8f4f8;
            border-left: 5px solid #3498db;
            padding: 15px 20px;
            margin-bottom: 20px;
            border-radius: 0 8px 8px 0;
        }
        .explanation h3 {
            margin-top: 0;
            color: #2980b9;
        }
        img {
            display: block;
            max-width: 100%;
            height: auto;
            border: 1px solid #ddd;
            border-radius: 5px;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
            margin: 0 auto;
        }
        code {
            background-color: #f7f7f7;
            padding: 2px 5px;
            border-radius: 3px;
            font-family: Consolas, monospace;
            color: #c7254e;
        }
    </style>
</head>
<body>

<div class="container">
    <h1>Documentación: Proceso de Fine-Tuning con MLX</h1>

    <!-- PASO 1 -->
    <div class="step">
        <div class="explanation">
            <h3>1. Preparación del entorno Python</h3>
            <p>Comenzamos creando un directorio de trabajo limpio y eliminando versiones anteriores si existen. Se configura un entorno virtual (<code>python3 -m venv venv</code>) para aislar las dependencias del proyecto y no afectar al sistema operativo base. Esto es una buena práctica estándar en desarrollo Python.</p>
        </div>
        <img src="Imagen1.png" alt="Captura 1">
    </div>

    <!-- PASO 2 -->
    <div class="step">
        <div class="explanation">
            <h3>2. Instalación de librerías MLX</h3>
            <p>Instalamos las librerías específicas de Apple: <code>mlx-lm</code> (para gestión de LLMs), <code>datasets</code> y <code>huggingface_hub</code>. MLX es el framework de Apple que sustituye a CUDA/PyTorch en los chips M1/M2/M3, permitiendo usar la memoria unificada del Mac para entrenar modelos localmente.</p>
        </div>
        <img src="Imagen2.png" alt="Captura 2">
    </div>

    <!-- PASO 3 -->
    <div class="step">
        <div class="explanation">
            <h3>3. Autenticación en Hugging Face</h3>
            <p>El sistema solicita un token de Hugging Face. Esto es necesario porque muchos modelos (incluso los abiertos como Gemma) requieren aceptar términos de uso. El comando <code>hf auth login</code> (o su versión moderna) conecta nuestra terminal con el repositorio para poder descargar los pesos del modelo.</p>
        </div>
        <img src="Imagen3.png" alt="Captura 3">
    </div>

    <!-- PASO 4 -->
    <div class="step">
        <div class="explanation">
            <h3>4. Creación del Dataset (Train)</h3>
            <p>Creamos manualmente el archivo <code>data/train.jsonl</code> usando el formato de chat (roles: user, assistant). Aquí definimos qué queremos que aprenda el modelo: preguntas sobre "ASIR", configuración de Apache y comandos Linux. Estos datos son la "verdad fundamental" (Ground Truth) del entrenamiento.</p>
        </div>
        <img src="Imagen4.png" alt="Captura 4">
    </div>

    <!-- PASO 5 -->
    <div class="step">
        <div class="explanation">
            <h3>5. Creación del Dataset (Validation)</h3>
            <p>Creamos el archivo de validación <code>data/valid.jsonl</code>. Este archivo es crítico: el modelo nunca entrena con él, solo lo usa para examinarse a sí mismo tras cada época. Si el error baja aquí, significa que el modelo está aprendiendo de verdad y no solo memorizando.</p>
        </div>
        <img src="Imagen5.png" alt="Captura 5">
    </div>

    <!-- PASO 6 -->
    <div class="step">
        <div class="explanation">
            <h3>6. Error de dimensiones (Batch Size)</h3>
            <p>Al intentar entrenar, el sistema lanza un <code>ValueError</code>. El error indica que el dataset debe tener al menos tantos ejemplos como el tamaño del lote (<code>batch_size=2</code>). Como nuestro archivo de validación solo tenía 1 línea, el proceso falla matemáticamente al intentar dividir los datos.</p>
        </div>
        <img src="Imagen6.png" alt="Captura 6">
    </div>

    <!-- PASO 7 -->
    <div class="step">
        <div class="explanation">
            <h3>7. Análisis del error (Traceback)</h3>
            <p>El "Traceback" detallado confirma que el fallo ocurre en el iterador de datos (<code>tqdm/std.py</code> y <code>trainer.py</code>). El cargador de datos intenta coger 2 elementos, encuentra solo 1 y colapsa. Esto nos obliga a corregir los hiperparámetros o ampliar los datos.</p>
        </div>
        <img src="Imagen7.png" alt="Captura 7">
    </div>

    <!-- PASO 8 -->
    <div class="step">
        <div class="explanation">
            <h3>8. Solución del problema de datos</h3>
            <p>Corregimos el problema añadiendo más datos al dataset o ajustando la configuración. También verificamos la integridad de los archivos JSONL con <code>cat</code> para asegurarnos de que la sintaxis JSON es válida y no hay comillas rotas que confundan al parser.</p>
        </div>
        <img src="Imagen8.png" alt="Captura 8">
    </div>

    <!-- PASO 9 -->
    <div class="step">
        <div class="explanation">
            <h3>9. Verificación de Rutas y Archivos</h3>
            <p>Se comprueba que la estructura de carpetas es correcta (<code>ls data/</code>) y que el script de entrenamiento (<code>mlx_lm.lora</code>) es accesible desde el entorno virtual. A veces el error "command not found" aparece si no hemos activado el venv correctamente.</p>
        </div>
        <img src="Imagen9.png" alt="Captura 9">
    </div>

    <!-- PASO 10 -->
    <div class="step">
        <div class="explanation">
            <h3>10. Inicio exitoso del entrenamiento</h3>
            <p>Con los errores resueltos, el entrenamiento comienza. Se cargan los pesos del modelo pre-entrenado (Gemma-3-270m) y se inicializan los adaptadores LoRA. Vemos que los "parámetros entrenables" son una fracción muy pequeña del total, lo que confirma la eficiencia de LoRA.</p>
        </div>
        <img src="Imagen10.png" alt="Captura 10">
    </div>

    <!-- PASO 11 -->
    <div class="step">
        <div class="explanation">
            <h3>11. Progreso del Fine-Tuning (Loss)</h3>
            <p>La métrica clave aquí es <code>Train loss</code>. Empieza alta (11.509) y baja rápidamente hasta 0.334 en la iteración 50. Esta caída drástica indica que el modelo ha "entendido" perfectamente nuestros ejemplos y ahora predice las respuestas de ASIR con alta confianza.</p>
        </div>
        <img src="Imagen11.png" alt="Captura 11">
    </div>

    <!-- PASO 12 -->
    <div class="step">
        <div class="explanation">
            <h3>12. Selección de modelo cuantizado</h3>
            <p>Para la inferencia, podemos elegir versiones optimizadas. La lista muestra variantes de Gemma (4bit, 8bit, bf16). Usar la versión de 4 bits es ideal para MacBooks con poca RAM, ya que mantiene la calidad casi intacta ocupando la mitad de memoria.</p>
        </div>
        <img src="Imagen12.png" alt="Captura 12">
    </div>

    <!-- PASO 13 -->
    <div class="step">
        <div class="explanation">
            <h3>13. Prueba de inferencia (Antes vs Después)</h3>
            <p>Aquí comparamos resultados. El modelo base (arriba) da una definición genérica o corta. El modelo afinado (abajo, <code>gemma-asir-fused</code>) responde usando el estilo y contenido exacto que le enseñamos, definiendo "fine-tuning" como la adaptación de modelos con LoRA.</p>
        </div>
        <img src="Imagen13.png" alt="Captura 13">
    </div>

    <!-- PASO 14 -->
    <div class="step">
        <div class="explanation">
            <h3>14. Validación final en LM Studio</h3>
            <p>Cargamos el modelo final en una interfaz gráfica (LM Studio). Al preguntarle sobre Apache en Ubuntu o comandos de Linux, el modelo responde correctamente. Esto demuestra que los conocimientos técnicos inyectados durante el entrenamiento se han consolidado y son utilizables en un chat real.</p>
        </div>
        <img src="Imagen14.png" alt="Captura 14">
    </div>

</div>

</body>
</html>

