<!DOCTYPE html>
<html lang="es">
<head>
  <meta charset="UTF-8">
  <title>Fine-tuning de Gemma 3-270M-it con MLX-LM-LoRA</title>
  <style>
    body {
      font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
      margin: 0;
      padding: 2rem;
      background-color: #0b1020;
      color: #f4f4f4;
    }
    h1, h2 {
      text-align: center;
    }
    .step {
      border: 1px solid #444;
      border-radius: 8px;
      padding: 1rem 1.5rem;
      margin: 1.5rem auto;
      max-width: 960px;
      background: #151a2a;
      box-shadow: 0 2px 8px rgba(0,0,0,0.4);
    }
    .step h2 {
      margin-top: 0;
      font-size: 1.2rem;
      color: #ffe28a;
    }
    .step p {
      line-height: 1.5;
    }
    .step img {
      display: block;
      max-width: 100%;
      height: auto;
      margin: 0.75rem auto;
      border-radius: 6px;
      border: 1px solid #333;
    }
    .caption {
      font-size: 0.9rem;
      color: #c7c7c7;
      text-align: center;
      margin-top: 0.25rem;
    }
    .small-note {
      font-size: 0.8rem;
      text-align: center;
      color: #aaaaaa;
    }
  </style>
</head>
<body>

  <h1>Memoria de Fine-tuning<br>Gemma 3‑270M‑it con MLX‑LM‑LoRA</h1>
  <p class="small-note">
    Secuencia de pasos desde la creación del entorno hasta el entrenamiento exitoso del modelo.
  </p>

  <!-- Paso 1 -->
  <section class="step">
    <h2>Paso 1 – Creación del entorno e instalación de dependencias</h2>
    <p>
      En este primer paso creo el directorio de trabajo <code>gemma-finetune-v2</code>, 
      genero y activo el entorno virtual de Python y actualizo <code>pip</code>. 
      A continuación instalo los paquetes necesarios: <code>mlx-lm</code>, 
      <code>datasets</code>, <code>huggingface_hub</code> y <code>accelerate</code>, 
      que permiten cargar y afinar el modelo Gemma en Apple Silicon.
    </p>
    <img src="WhatsApp-Image-2026-02-06-at-16.28.38.jpg" alt="Instalación de entorno y dependencias">
    <div class="caption">Instalación del entorno virtual y de las librerías necesarias para MLX‑LM.</div>
  </section>

  <!-- Paso 2 -->
  <section class="step">
    <h2>Paso 2 – Autenticación en Hugging Face</h2>
    <p>
      Después configuro el acceso a Hugging Face ejecutando <code>huggingface-cli login</code>. 
      Introduzco el token personal para poder descargar el modelo base Gemma 
      y, si quiero, subir más tarde los pesos afinados al Hub.
    </p>
    <img src="WhatsApp-Image-2026-02-06-at-16.28.39-3.jpg" alt="Login en Hugging Face">
    <div class="caption">Proceso de login en Hugging Face desde la terminal.</div>
  </section>

  <!-- Paso 3 -->
  <section class="step">
    <h2>Paso 3 – Primeros ficheros de datos en JSONL</h2>
    <p>
      Creo el directorio <code>data/</code> y genero los ficheros <code>train.jsonl</code> y 
      <code>valid.jsonl</code> con los primeros ejemplos de conversación en formato 
      chat (<code>{"messages": [...]}</code>). Uso el comando <code>head</code> para 
      comprobar el contenido, aunque en este intento aparece un error al interpretar 
      el comentario <code># Verifica</code> como nombre de archivo.
    </p>
    <img src="WhatsApp-Image-2026-02-06-at-16.28.40-3.jpg" alt="Creación inicial de train y valid JSONL">
    <div class="caption">Generación manual de los primeros ficheros <code>train.jsonl</code> y <code>valid.jsonl</code>.</div>
  </section>

  <!-- Paso 4 -->
  <section class="step">
    <h2>Paso 4 – Intento de lanzar el comando de LoRA</h2>
    <p>
      Intento iniciar el entrenamiento ejecutando <code>mlx_lm.lora</code> con el modelo 
      <code>google/gemma-3-270m-it</code>, apuntando a la carpeta <code>data/</code> y 
      definiendo iteraciones, batch size y ruta de los adaptadores. 
      Sin embargo, el shell devuelve <code>command not found</code> porque el ejecutable correcto 
      del paquete <code>mlx-lm-lora</code> es <code>mlx_lm_lora.train</code>.
    </p>
    <img src="WhatsApp-Image-2026-02-06-at-16.28.40-2.jpg" alt="Comando mlx_lm.lora no encontrado">
    <div class="caption">Primer intento fallido de lanzar el entrenamiento con el comando incorrecto.</div>
  </section>

  <!-- Paso 5 -->
  <section class="step">
    <h2>Paso 5 – Creación de un archivo de configuración YAML</h2>
    <p>
      Para simplificar los parámetros preparo un fichero <code>train.yaml</code> donde 
      defino el modelo, la ruta de datos, el número de iteraciones, el <code>batch_size</code> 
      y la ruta de salida de los adaptadores LoRA. Aun así, al ejecutar 
      <code>mlx_lm.lora --config train.yaml</code> el sistema sigue sin encontrar el comando, 
      lo que confirma que debo usar la interfaz oficial <code>mlx_lm_lora.train</code>.
    </p>
    <img src="WhatsApp-Image-2026-02-06-at-16.28.40-1.jpg" alt="Creación de train.yaml">
    <div class="caption">Definición de la configuración de entrenamiento en formato YAML.</div>
  </section>

  <!-- Paso 6 -->
  <section class="step">
    <h2>Paso 6 – Primer entrenamiento: error por tamaño de dataset</h2>
    <p>
      Una vez identificado el ejecutable correcto, lanzo el entrenamiento con 
      <code>mlx_lm_lora.train</code> y se empieza a cargar el modelo Gemma. 
      No obstante, durante la fase de evaluación aparece el mensaje 
      <code>ValueError: Dataset must have at least batch_size*2 examples but only has 1</code>, 
      indicando que el conjunto de validación es demasiado pequeño para el <code>batch_size</code> elegido.
    </p>
    <img src="WhatsApp-Image-2026-02-06-at-16.28.39-2.jpg" alt="Error de dataset vs batch_size">
    <div class="caption">El entrenamiento se detiene porque el conjunto de validación sólo tiene un ejemplo.</div>
  </section>

  <!-- Paso 7 -->
  <section class="step">
    <h2>Paso 7 – Regeneración de los ficheros de entrenamiento y validación</h2>
    <p>
      Para solucionar el problema elimino el directorio <code>data</code>, lo vuelvo a crear 
      y genero de nuevo <code>train.jsonl</code> con varios mensajes de usuario y respuestas 
      del asistente centrados en ASIR, Apache y fine‑tuning. 
      También creo un <code>valid.jsonl</code> con al menos un ejemplo adicional, y reviso el contenido 
      con <code>cat</code> para asegurarme de que las líneas tienen formato JSONL correcto.
    </p>
    <img src="WhatsApp-Image-2026-02-06-at-16.28.39-1.jpg" alt="Regeneración de datos JSONL">
    <div class="caption">Nueva generación de los ficheros de datos con más ejemplos y formato correcto.</div>
  </section>

  <!-- Paso 8 -->
  <section class="step">
    <h2>Paso 8 – Carga del dataset: error de índice fuera de rango</h2>
    <p>
      Al relanzar el entrenamiento, el script descarga los ficheros del modelo pero falla 
      al cargar el dataset local con el error <code>IndexError: list index out of range</code>. 
      Este error se produce cuando el código intenta acceder a <code>data[0]</code> y la lista está vacía, 
      lo que indica que la carpeta <code>data/</code> o las líneas del <code>.jsonl</code> aún no se están leyendo correctamente.
    </p>
    <img src="WhatsApp-Image-2026-02-06-at-16.28.41.jpg" alt="Error IndexError al cargar el dataset">
    <div class="caption">Fallo al crear el dataset a partir de los ficheros JSONL locales.</div>
  </section>

  <!-- Paso 9 -->
  <section class="step">
    <h2>Paso 9 – Ajuste del contenido de los JSONL y nuevo intento</h2>
    <p>
      Vuelvo a editar <code>train.jsonl</code> y <code>valid.jsonl</code>, manteniendo un ejemplo por línea 
      y el formato de mensajes requerido por <code>mlx-lm-lora</code>. 
      Relanzo el entrenamiento con los mismos parámetros (modelo Gemma, 50 iteraciones, 
      <code>batch_size 2</code> y ruta de adaptadores), pero el validador aún detecta que 
      el conjunto de validación tiene menos ejemplos de los necesarios para el batch.
    </p>
    <img src="WhatsApp-Image-2026-02-06-at-16.28.41-1.jpg" alt="Nuevo intento de entrenamiento después de ajustar los JSONL">
    <div class="caption">Relanzando el entrenamiento tras corregir la estructura de los ficheros de datos.</div>
  </section>

  <!-- Paso 10 -->
  <section class="step">
    <h2>Paso 10 – Ampliación definitiva de train y valid</h2>
    <p>
      Finalmente amplío el número de ejemplos tanto en <code>train.jsonl</code> como en 
      <code>valid.jsonl</code>, añadiendo nuevas instrucciones sobre bases de datos 
      relacionales y comandos básicos de Linux. 
      Con ello garantizo que el conjunto de validación tiene, como mínimo, 
      <code>batch_size*2</code> ejemplos, condición que exige el entrenador de MLX‑LM‑LoRA.
    </p>
    <img src="WhatsApp-Image-2026-02-06-at-16.28.41-2.jpg" alt="Ampliación del dataset de validación">
    <div class="caption">Ampliación del dataset con más ejemplos para evitar errores en la evaluación.</div>
  </section>

  <!-- Paso 11 -->
  <section class="step">
    <h2>Paso 11 – Entrenamiento exitoso del modelo Gemma afinado</h2>
    <p>
      Con los datasets ya corregidos, ejecuto de nuevo <code>mlx_lm_lora.train</code> 
      sobre el modelo <code>google/gemma-3-270m-it</code>, durante 50 iteraciones y 
      <code>batch_size 2</code>. 
      El log muestra la evolución de la pérdida de entrenamiento y validación y, al finalizar, 
      se guardan los pesos LoRA en <code>my-gemma-lora/adapters.safetensors</code>, 
      dejando listo el modelo afinado para su uso con <code>mlx_lm.generate</code>.
    </p>
    <img src="WhatsApp-Image-2026-02-06-at-16.28.40.jpg" alt="Entrenamiento exitoso de Gemma 3-270M-it con LoRA">
    <div class="caption">Entrenamiento completado y guardado de los adaptadores LoRA del modelo Gemma.</div>
  </section>

  <!-- Paso 12 -->
  <section class="step">
    <h2>Paso 12 – Modelos Gemma 3‑270M‑it en formato MLX</h2>
    <p>
      Como referencia de la arquitectura, consulto la lista de variantes de 
      <code>gemma-3-270m-it</code> disponibles en formato MLX (bf16, 4‑bit, 8‑bit, etc.) 
      en el repositorio de la comunidad. 
      Esta información me permite elegir la versión adecuada del modelo base para el fine‑tuning local.
    </p>
    <img src="WhatsApp-Image-2026-02-06-at-16.28.39.jpg" alt="Listado de variantes Gemma 3-270m-it MLX">
    <div class="caption">Variantes del modelo Gemma 3‑270M‑it listas para usar con MLX‑LM.</div>
  </section>

  <!-- Paso opcional: mini icono -->
  <section class="step">
    <h2>Paso opcional – Miniatura / icono del proyecto</h2>
    <p>
      Además incluyo una pequeña imagen que puedo utilizar como icono o elemento decorativo
      en la portada del proyecto de fine‑tuning.
    </p>
    <img src="WhatsApp-Image-2026-02-06-at-16.28.41-3.jpg" alt="Miniatura del proyecto">
    <div class="caption">Miniatura asociada al trabajo de fine‑tuning.</div>
  </section>

</body>
</html>
