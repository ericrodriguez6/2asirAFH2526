<!DOCTYPE html>
<html lang="es">
<head>
  <meta charset="UTF-8">
  <title>Práctica de Cuantización de Modelos LLM</title>
  <style>
    body {
      font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
      margin: 0;
      padding: 2rem;
      background-color: #0b1020;
      color: #f4f4f4;
    }
    h1, h2 {
      text-align: center;
    }
    .step {
      border: 1px solid #444;
      border-radius: 8px;
      padding: 1rem 1.5rem;
      margin: 1.5rem auto;
      max-width: 960px;
      background: #151a2a;
      box-shadow: 0 2px 8px rgba(0,0,0,0.4);
    }
    .step h2 {
      margin-top: 0;
      font-size: 1.2rem;
      color: #ffe28a;
    }
    .step p {
      line-height: 1.5;
    }
    .step img {
      display: block;
      max-width: 100%;
      height: auto;
      margin: 0.75rem auto;
      border-radius: 6px;
      border: 1px solid #333;
    }
    .caption {
      font-size: 0.9rem;
      color: #c7c7c7;
      text-align: center;
      margin-top: 0.25rem;
    }
    .small-note {
      font-size: 0.8rem;
      text-align: center;
      color: #aaaaaa;
    }
  </style>
</head>
<body>

  <h1>Memoria de Práctica:<br>Fusión y Cuantización de Modelos LLM</h1>
  <p class="small-note">
    Proceso desde la fusión de adaptadores LoRA hasta la conversión a GGUF y cuantización en 4 bits.
  </p>

  <!-- Paso 1 -->
  <section class="step">
    <h2>Paso 1 – Fusión de adaptadores LoRA</h2>
    <p>
      Comienzo fusionando los pesos entrenados (LoRA) con el modelo base usando de>mlx_lm.fuse</code>. 
      Esto genera un modelo completo listo para ser convertido.
    </p>
    <!-- 16.29.04 (3) -->
    <img src="WhatsApp%20Image%202026-02-06%20at%2016.29.04%20(3).jpeg" alt="Fusión del modelo con mlx_lm.fuse">
    <div class="caption">Ejecución del comando de fusión para integrar los pesos LoRA.</div>
  </section>

  <!-- Paso 2 -->
  <section class="step">
    <h2>Paso 2 – Instalación de llama.cpp</h2>
    <p>
      Para trabajar con el formato GGUF necesito de>llama.cpp</code>. 
      Clono el repositorio oficial e instalo los requerimientos de Python.
    </p>
    <!-- 16.29.04 (1) -->
    <img src="WhatsApp%20Image%202026-02-06%20at%2016.29.04%20(1).jpeg" alt="Clonado e instalación de llama.cpp">
    <div class="caption">Preparación del entorno de llama.cpp.</div>
  </section>

  <!-- Paso 3 -->
  <section class="step">
    <h2>Paso 3 – Conversión a formato GGUF (FP16)</h2>
    <p>
      Con el script de>convert_hf_to_gguf.py</code> convierto el modelo fusionado 
      al formato GGUF con precisión completa. El archivo resultante pesa 1.2 GB.
    </p>
    <!-- 16.29.03 -->
    <img src="WhatsApp%20Image%202026-02-06%20at%2016.29.03.jpeg" alt="Conversión de HF a GGUF">
    <div class="caption">Conversión del modelo fusionado a GGUF sin comprimir.</div>
  </section>

  <!-- Paso 4 -->
  <section class="step">
    <h2>Paso 4 – Cuantización a 4 bits (Q4_K_M)</h2>
    <p>
      Aplico la cuantización usando de>llama-quantize</code>. Selecciono el método 
      de>q4_k_m</code>, reduciendo el tamaño a 469 MB.
    </p>
    <!-- 16.29.04 (2) -->
    <img src="WhatsApp%20Image%202026-02-06%20at%2016.29.04%20(2).jpeg" alt="Proceso de cuantización a Q4_K_M">
    <div class="caption">Log del proceso de cuantización reduciendo los tensores.</div>
  </section>

  <!-- Paso 5 -->
  <section class="step">
    <h2>Paso 5 – Comparación de tamaños</h2>
    <p>
      Verifico la diferencia: el original FP16 ocupa 1.2 GB, mientras que la versión 
      cuantizada de>q4_k_m</code> ocupa solo 469 MB.
    </p>
    <!-- 16.29.05 (1) -->
    <img src="WhatsApp%20Image%202026-02-06%20at%2016.29.05%20(1).jpeg" alt="Comparación de tamaño de archivos GGUF">
    <div class="caption">Diferencia de peso entre el modelo FP16 y el cuantizado Q4.</div>
  </section>

  <!-- Paso 6 -->
  <section class="step">
    <h2>Paso 6 – Prueba del modelo original (Referencia)</h2>
    <p>
      Lanzo el modelo original en de>llama-cli</code> para tener una referencia de calidad.
    </p>
    <!-- 16.29.05 (3) -->
    <img src="WhatsApp%20Image%202026-02-06%20at%2016.29.05%20(3).jpeg" alt="Prueba de inferencia con modelo original">
    <div class="caption">Inferencia de prueba con el modelo base.</div>
  </section>

  <!-- Paso 7 -->
  <section class="step">
    <h2>Paso 7 – Prueba del modelo cuantizado</h2>
    <p>
      Ejecuto el modelo comprimido (de>q4_k_m.gguf</code>). Responde correctamente 
      y más rápido a la pregunta sobre el servidor DNS.
    </p>
    <!-- 16.29.04 -->
    <img src="WhatsApp%20Image%202026-02-06%20at%2016.29.04.jpeg" alt="Prueba de inferencia con modelo cuantizado">
    <div class="caption">El modelo cuantizado responde correctamente ocupando menos memoria.</div>
  </section>

  <!-- Paso 8 -->
  <section class="step">
    <h2>Paso 8 – Comparativa de respuestas detallada</h2>
    <p>
      Prueba extensa sobre "administración de sistemas" con el modelo original.
    </p>
    <!-- 16.29.05 (2) -->
    <img src="WhatsApp%20Image%202026-02-06%20at%2016.29.05%20(2).jpeg" alt="Respuesta detallada modelo original">
    <div class="caption">Generación de texto largo con el modelo base.</div>
  </section>

  <!-- Paso 9 -->
  <section class="step">
    <h2>Paso 9 – Respuesta detallada del modelo cuantizado</h2>
    <p>
      El modelo cuantizado genera la misma explicación técnica, confirmando que la 
      pérdida de precisión es aceptable.
    </p>
    <!-- 16.29.05 -->
    <img src="WhatsApp%20Image%202026-02-06%20at%2016.29.05.jpeg" alt="Respuesta detallada modelo cuantizado">
    <div class="caption">El modelo comprimido mantiene la coherencia.</div>
  </section>

</body>
</html>


