<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Práctica: Cuantización de LLMs con llama.cpp</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <style>
        body { background-color: #f8f9fa; }
        .header-section { background: linear-gradient(135deg, #6a11cb 0%, #2575fc 100%); color: white; padding: 2rem 0; margin-bottom: 2rem; border-radius: 0 0 15px 15px; }
        .card { margin-bottom: 2rem; box-shadow: 0 4px 6px rgba(0,0,0,0.1); border: none; }
        .card-header { background-color: #ffffff; border-bottom: 2px solid #f0f0f0; font-weight: bold; color: #333; }
        .img-preview { width: 100%; border-radius: 8px; border: 1px solid #ddd; margin-bottom: 10px; transition: transform 0.2s; }
        .img-preview:hover { transform: scale(1.01); box-shadow: 0 10px 20px rgba(0,0,0,0.15); }
        .metric-box { background-color: #e9ecef; padding: 10px; border-radius: 5px; text-align: center; font-weight: bold; }
        .metric-good { color: #198754; } /* Verde para buenos resultados */
        .metric-base { color: #0d6efd; } /* Azul para base */
    </style>
</head>
<body>

    <!-- Encabezado -->
    <div class="header-section text-center">
        <div class="container">
            <h1 class="display-4">Práctica de Cuantización</h1>
            <p class="lead">Optimización de Modelos de Lenguaje (LLM) con llama.cpp</p>
            <p class="small">Administración de Sistemas Informáticos en Red (ASIR)</p>
        </div>
    </div>

    <div class="container">

        <!-- INTRODUCCIÓN -->
        <div class="card">
            <div class="card-body">
                <h5 class="card-title">Objetivo de la práctica</h5>
                <p class="card-text">
                    El objetivo es reducir el tamaño y consumo de memoria de un modelo de Inteligencia Artificial (Qwen2.5-0.5B) mediante la técnica de <strong>cuantización</strong>. 
                    Pasaremos de una precisión de 16 bits (FP16) a 4 bits (Q4_K_M) y compararemos el rendimiento.
                </p>
            </div>
        </div>

        <!-- SECCIÓN 1: PROCESO TÉCNICO -->
        <div class="card">
            <div class="card-header">1. Proceso de Cuantización (Backend)</div>
            <div class="card-body">
                <div class="row">
                    <!-- Imagen 1: Ejecución -->
                    <div class="col-md-6 mb-4">
                        <h6>Ejecución del algoritmo</h6>
                        <img src="Imagen1Cuantizacion.jpg" alt="Ejecución llama-quantize" class="img-preview">
                        <p class="text-muted small">
                            Ejecución del comando <code>llama-quantize</code>. Se observa cómo el script procesa los tensores capa por capa, convirtiendo los pesos de <code>F16</code> a <code>Q4_K</code>.
                        </p>
                    </div>
                    <!-- Imagen 4: Archivos -->
                    <div class="col-md-6 mb-4">
                        <h6>Resultado en disco</h6>
                        <img src="Imagen4Cuantizacion.jpg" alt="Comparativa de archivos" class="img-preview">
                        <div class="metric-box mt-2">
                            <span class="d-block text-muted">Reducción de tamaño:</span>
                            <span class="metric-base">Original: 1.2 GB</span> <i class="bi bi-arrow-right"></i> 
                            <span class="metric-good">Cuantizado: 469 MB</span>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- SECCIÓN 2: RENDIMIENTO -->
        <div class="card">
            <div class="card-header">2. Comparativa de Rendimiento (Inferencia en Terminal)</div>
            <div class="card-body">
                <p>Se realiza un benchmark de velocidad generando texto con ambos modelos en la terminal.</p>
                <div class="row">
                    <!-- Imagen 3: FP16 -->
                    <div class="col-md-6 mb-4">
                        <h6>Modelo Original (FP16)</h6>
                        <img src="Imagen3Cuantizacion.jpg" alt="Benchmark FP16" class="img-preview">
                        <p class="text-danger fw-bold">Velocidad: 95.2 tokens/seg</p>
                        <p class="small">El modelo base es más lento debido al mayor ancho de banda de memoria necesario.</p>
                    </div>
                    <!-- Imagen 2: Q4 -->
                    <div class="col-md-6 mb-4">
                        <h6>Modelo Cuantizado (Q4)</h6>
                        <img src="Imagen2Cuantizacion.jpg" alt="Benchmark Q4" class="img-preview">
                        <p class="text-success fw-bold">Velocidad: 174.6 tokens/seg</p>
                        <p class="small">El modelo optimizado casi duplica la velocidad de inferencia.</p>
                    </div>
                </div>
            </div>
        </div>

        <!-- SECCIÓN 3: CALIDAD -->
        <div class="card">
            <div class="card-header">3. Verificación de Calidad (LM Studio)</div>
            <div class="card-body">
                <p>Carga de modelos en entorno gráfico para verificar la coherencia de las respuestas.</p>
                <div class="row">
                    <!-- Imagen 5: Chat FP16 -->
                    <div class="col-md-12 mb-4">
                        <h6>Respuesta Modelo Original</h6>
                        <img src="Imagen5Cuantizacion.jpg" alt="Chat FP16" class="img-preview">
                        <p class="small text-muted">Respuesta coherente y detallada sobre administración de sistemas.</p>
                    </div>
                    <!-- Imagen 6: Chat Q4 -->
                    <div class="col-md-12 mb-4">
                        <h6>Respuesta Modelo Cuantizado</h6>
                        <img src="Imagen6Cuantizacion.jpg" alt="Chat Q4" class="img-preview">
                        <p class="small text-muted">Respuesta igualmente válida. La cuantización ha mantenido la capacidad de razonamiento del modelo.</p>
                    </div>
                </div>
            </div>
        </div>

        <!-- CONCLUSIÓN -->
        <div class="alert alert-success text-center" role="alert">
            <h4 class="alert-heading">Conclusión</h4>
            <p>La práctica demuestra que la cuantización a 4 bits es altamente efectiva. Hemos logrado reducir el almacenamiento en un <strong>61%</strong> y aumentar la velocidad en un <strong>83%</strong>, sin pérdida perceptible en la calidad de las respuestas.</p>
        </div>

    </div>

    <footer class="text-center py-4 text-muted">
        <p>Práctica realizada por [Tu Nombre] - ASIR</p>
    </footer>

</body>
</html>
