<!DOCTYPE html>
<html lang="es">
<head>
  <meta charset="UTF-8">
  <title>Práctica de Cuantización con llama.cpp</title>
  <style>
    body {
      font-family: system-ui, -apple-system, sans-serif;
      margin: 0;
      padding: 2rem;
      background-color: #0d1117;
      color: #c9d1d9;
    }
    .container {
      max-width: 900px;
      margin: 0 auto;
    }
    h1 {
      text-align: center;
      color: #58a6ff;
      border-bottom: 1px solid #30363d;
      padding-bottom: 1rem;
    }
    .step {
      background: #161b22;
      border: 1px solid #30363d;
      border-radius: 6px;
      margin-bottom: 2rem;
      padding: 1.5rem;
    }
    .step h2 {
      margin-top: 0;
      color: #79c0ff;
      font-size: 1.3rem;
    }
    .step p {
      line-height: 1.6;
      margin-bottom: 1rem;
    }
    img {
      display: block;
      width: 100%;
      height: auto;
      border-radius: 4px;
      border: 1px solid #30363d;
    }
    .caption {
      font-size: 0.85rem;
      color: #8b949e;
      text-align: center;
      margin-top: 0.5rem;
      font-style: italic;
    }
    code {
      background: #232831;
      padding: 0.2rem 0.4rem;
      border-radius: 4px;
      font-family: monospace;
      font-size: 0.9em;
    }
  </style>
</head>
<body>

<div class="container">
  <h1>Optimización de LLMs: Fusión y Cuantización</h1>

  <!-- PASO 1 -->
  <div class="step">
    <h2>1. Fusión del Modelo (MLX)</h2>
    <p>El primer paso consiste en fusionar los adaptadores LoRA entrenados con el modelo base. Usamos el comando de>mlx_lm.fuse</code> para integrar los pesos y exportar el modelo completo (fused) que luego convertiremos.</p>
    <!-- Original: WhatsApp Image 2026-02-06 at 16.29.04 (3).jpeg -->
    <img src="WhatsApp%20Image%202026-02-06%20at%2016.29.04%20(3).jpeg" alt="Fusión del modelo">
    <div class="caption">Ejecución de mlx_lm.fuse para unir adaptadores y modelo base.</div>
  </div>

  <!-- PASO 2 -->
  <div class="step">
    <h2>2. Preparación del Entorno llama.cpp</h2>
    <p>Clonamos el repositorio de de>llama.cpp</code> e instalamos las dependencias necesarias. Esto nos permitirá ejecutar los scripts de conversión a GGUF y usar las herramientas de cuantización.</p>
    <!-- Original: WhatsApp Image 2026-02-06 at 16.29.04 (1).jpeg -->
    <img src="WhatsApp%20Image%202026-02-06%20at%2016.29.04%20(1).jpeg" alt="Instalación llama.cpp">
    <div class="caption">Setup inicial del repositorio llama.cpp.</div>
  </div>

  <!-- PASO 3 -->
  <div class="step">
    <h2>3. Conversión a formato GGUF (FP16)</h2>
    <p>Utilizamos el script de>convert_hf_to_gguf.py</code> para transformar nuestro modelo fusionado (formato HuggingFace) al formato GGUF estandarizado. En este punto, el modelo mantiene una precisión alta (probablemente FP16) y un tamaño grande.</p>
    <!-- Original: WhatsApp Image 2026-02-06 at 16.29.03.jpeg -->
    <img src="WhatsApp%20Image%202026-02-06%20at%2016.29.03.jpeg" alt="Conversión a GGUF">
    <div class="caption">Conversión del modelo fused a formato GGUF sin comprimir.</div>
  </div>

  <!-- PASO 4 -->
  <div class="step">
    <h2>4. Cuantización (Compresión a 4 bits)</h2>
    <p>Este es el paso crítico. Usamos la herramienta binaria de>llama-quantize</code> para reducir la precisión de los pesos del modelo a 4 bits (método de>q4_k_m</code>). El log muestra cómo procesa cada bloque del modelo reduciendo su tamaño drásticamente.</p>
    <!-- Original: WhatsApp Image 2026-02-06 at 16.29.04 (2).jpeg -->
    <img src="WhatsApp%20Image%202026-02-06%20at%2016.29.04%20(2).jpeg" alt="Proceso de Cuantización">
    <div class="caption">Log de ejecución de llama-quantize reduciendo el modelo.</div>
  </div>

  <!-- PASO 5 -->
  <div class="step">
    <h2>5. Verificación de Tamaños</h2>
    <p>Comparamos los archivos resultantes. Se observa que el modelo original (fp16) ocupa <strong>1.2 GB</strong>, mientras que la versión cuantizada (q4_k_m) se ha reducido a solo <strong>469 MB</strong>, logrando una gran compresión.</p>
    <!-- Original: WhatsApp Image 2026-02-06 at 16.29.05 (1).jpeg -->
    <img src="WhatsApp%20Image%202026-02-06%20at%2016.29.05%20(1).jpeg" alt="Comparación de tamaños">
    <div class="caption">Listado de archivos mostrando la reducción de tamaño.</div>
  </div>

  <!-- PASO 6 -->
  <div class="step">
    <h2>6. Prueba de Inferencia: Modelo Original</h2>
    <p>Ejecutamos el modelo original (no cuantizado) con de>llama-cli</code> para establecer una línea base. Le pedimos una definición corta ("Explica qué es un servidor DNS") para ver su velocidad y calidad.</p>
    <!-- Original: WhatsApp Image 2026-02-06 at 16.29.05 (3).jpeg -->
    <img src="WhatsApp%20Image%202026-02-06%20at%2016.29.05%20(3).jpeg" alt="Test Modelo Original">
    <div class="caption">Inferencia con el modelo FP16 (más lento/pesado).</div>
  </div>

  <!-- PASO 7 -->
  <div class="step">
    <h2>7. Prueba de Inferencia: Modelo Cuantizado</h2>
    <p>Realizamos la misma prueba con el modelo comprimido (de>q4_k_m</code>). El modelo responde correctamente a la pregunta sobre DNS, demostrando que la pérdida de calidad es mínima para instrucciones simples.</p>
    <!-- Original: WhatsApp Image 2026-02-06 at 16.29.04.jpeg -->
    <img src="WhatsApp%20Image%202026-02-06%20at%2016.29.04.jpeg" alt="Test Modelo Cuantizado">
    <div class="caption">Inferencia con el modelo Q4 (más rápido/ligero).</div>
  </div>

  <!-- PASO 8 -->
  <div class="step">
    <h2>8. Generación de Texto Largo (Original)</h2>
    <p>Sometemos al modelo original a una prueba más exigente: explicar la "administración de sistemas". Genera una respuesta detallada y estructurada por puntos.</p>
    <!-- Original: WhatsApp Image 2026-02-06 at 16.29.05 (2).jpeg -->
    <img src="WhatsApp%20Image%202026-02-06%20at%2016.29.05%20(2).jpeg" alt="Texto Largo Original">
    <div class="caption">Respuesta compleja generada por el modelo base.</div>
  </div>

  <!-- PASO 9 -->
  <div class="step">
    <h2>9. Generación de Texto Largo (Cuantizado)</h2>
    <p>Finalmente, pedimos lo mismo al modelo cuantizado. La respuesta es igualmente detallada y coherente, confirmando que la cuantización a 4 bits es una estrategia efectiva para ahorrar recursos sin sacrificar demasiado conocimiento.</p>
    <!-- Original: WhatsApp Image 2026-02-06 at 16.29.05.jpeg -->
    <img src="WhatsApp%20Image%202026-02-06%20at%2016.29.05.jpeg" alt="Texto Largo Cuantizado">
    <div class="caption">Respuesta compleja generada por el modelo comprimido.</div>
  </div>

</div>

</body>
</html>

