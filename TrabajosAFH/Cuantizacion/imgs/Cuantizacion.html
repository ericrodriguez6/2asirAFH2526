<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Práctica: Cuantización de LLMs con llama.cpp - ASIR</title>
    <!-- Bootstrap 5 CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <style>
        body {
            background-color: #f4f6f9;
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            color: #333;
        }
        .header-section {
            background: linear-gradient(135deg, #0f2027, #203a43, #2c5364); /* Degradado oscuro profesional */
            color: white;
            padding: 3rem 0;
            margin-bottom: 3rem;
            box-shadow: 0 4px 15px rgba(0,0,0,0.2);
        }
        .card {
            border: none;
            border-radius: 12px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.05);
            margin-bottom: 2.5rem;
            transition: transform 0.2s;
        }
        .card:hover {
            transform: translateY(-5px);
            box-shadow: 0 12px 20px rgba(0,0,0,0.1);
        }
        .card-header {
            background-color: #fff;
            border-bottom: 2px solid #e9ecef;
            font-weight: 700;
            font-size: 1.2rem;
            color: #2c3e50;
            padding: 1.2rem 1.5rem;
        }
        .card-body {
            padding: 1.5rem;
        }
        .img-container {
            border: 1px solid #dee2e6;
            padding: 5px;
            border-radius: 8px;
            background-color: #fff;
            margin-bottom: 1rem;
        }
        .img-fluid {
            border-radius: 5px;
            width: 100%; /* Asegura que la imagen ocupe el ancho */
            height: auto;
        }
        .explanation-text {
            font-size: 0.95rem;
            line-height: 1.6;
            color: #555;
            text-align: justify;
        }
        .highlight {
            color: #0d6efd;
            font-weight: 600;
        }
        .metric-badge {
            font-size: 0.9rem;
            padding: 0.5em 0.8em;
            border-radius: 20px;
        }
        footer {
            background-color: #2c3e50;
            color: #bdc3c7;
            padding: 2rem 0;
            margin-top: 3rem;
        }
    </style>
</head>
<body>

    <!-- Header -->
    <header class="header-section text-center">
        <div class="container">
            <h1 class="display-5 fw-bold mb-3">Optimización de Modelos LLM</h1>
            <p class="lead mb-0">Práctica de Cuantización con <code>llama.cpp</code></p>
            <hr class="my-4 mx-auto" style="width: 100px; border-color: rgba(255,255,255,0.5);">
            <p class="small text-uppercase tracking-wide">Administración de Sistemas Informáticos en Red (ASIR)</p>
        </div>
    </header>

    <div class="container">

        <!-- INTRODUCCIÓN -->
        <div class="row justify-content-center">
            <div class="col-lg-10">
                <div class="card mb-5">
                    <div class="card-body">
                        <h4 class="card-title text-primary mb-3"><i class="bi bi-info-circle"></i> Objetivo de la Práctica</h4>
                        <p class="card-text explanation-text">
                            En el despliegue de Inteligencia Artificial local, el consumo de memoria VRAM y el ancho de banda son los principales cuellos de botella. 
                            El objetivo de esta práctica es aplicar técnicas de <strong>cuantización (quantization)</strong> para reducir la precisión de los pesos de un modelo LLM (Qwen2.5-0.5B).
                            Pasaremos de un formato de coma flotante de 16 bits (FP16) a un formato entero de 4 bits (Q4_K_M), analizando el impacto en tres dimensiones clave:
                            <strong>espacio en disco, velocidad de inferencia y calidad de respuesta</strong>.
                        </p>
                    </div>
                </div>
            </div>
        </div>

        <!-- SECCIÓN 1: PROCESO TÉCNICO -->
        <div class="row">
            <div class="col-12">
                <h3 class="mb-4 text-secondary border-start border-4 border-primary ps-3">1. Proceso de Cuantización (Backend)</h3>
            </div>
            
            <!-- Paso 1: Ejecución -->
            <div class="col-lg-6">
                <div class="card h-100">
                    <div class="card-header">Ejecución del Algoritmo</div>
                    <div class="card-body">
                        <div class="img-container">
                            <img src="Imagen1Cuantizacion.jpg" class="img-fluid" alt="Proceso de cuantización en terminal">
                        </div>
                        <h6 class="fw-bold mt-3">Análisis Técnico</h6>
                        <p class="explanation-text">
                            Se ejecuta el binario <code>llama-quantize</code>. El log muestra cómo el algoritmo recorre la red neuronal capa por capa (tensores).
                            Se observa la conversión de bloques de datos: los pesos originales en <code>F16</code> (16 bits) son transformados a bloques cuantizados <code>Q4_K</code>.
                            Esta operación reduce la precisión numérica de cada peso, agrupándolos en bloques para minimizar la pérdida de información (error de cuantización).
                            El proceso completo para este modelo tomó <strong>1414 ms</strong>.
                        </p>
                    </div>
                </div>
            </div>

            <!-- Paso 2: Resultado Archivos -->
            <div class="col-lg-6">
                <div class="card h-100">
                    <div class="card-header">Verificación de Almacenamiento</div>
                    <div class="card-body">
                        <div class="img-container">
                            <img src="Imagen4Cuantizacion.jpg" class="img-fluid" alt="Comparativa de tamaño de archivos">
                        </div>
                        <h6 class="fw-bold mt-3">Resultado de la Compresión</h6>
                        <p class="explanation-text">
                            Al listar el directorio tras la operación, se evidencia una reducción drástica del almacenamiento requerido.
                            El archivo original <code>fp16.gguf</code> ocupa <strong>1,2 GB</strong>, mientras que el resultante <code>q4_k_m.gguf</code> ocupa solo <strong>469 MB</strong>.
                            Esto representa una reducción del <strong>~61%</strong> en el tamaño del archivo, lo que facilita su distribución y permite cargarlo en dispositivos con menos memoria RAM/VRAM.
                        </p>
                    </div>
                </div>
            </div>
        </div>

        <!-- SECCIÓN 2: RENDIMIENTO -->
        <div class="row mt-5">
            <div class="col-12">
                <h3 class="mb-4 text-secondary border-start border-4 border-success ps-3">2. Comparativa de Rendimiento (Inferencia)</h3>
            </div>

            <!-- Benchmark FP16 -->
            <div class="col-lg-6">
                <div class="card border-primary h-100" style="border-width: 0 0 0 2px;">
                    <div class="card-body">
                        <h5 class="text-primary fw-bold">Modelo Base (FP16)</h5>
                        <div class="img-container">
                            <img src="Imagen3Cuantizacion.jpg" class="img-fluid" alt="Benchmark FP16">
                        </div>
                        <p class="explanation-text">
                            Al ejecutar una prueba de generación de texto ("Explica qué es un servidor DNS"), el modelo base alcanza una velocidad de 
                            <span class="badge bg-primary metric-badge">95,2 tokens/seg</span>. 
                            Aunque es rápido, el ancho de banda de memoria necesario para mover datos de 16 bits limita su rendimiento máximo en hardware estándar.
                        </p>
                    </div>
                </div>
            </div>

            <!-- Benchmark Q4 -->
            <div class="col-lg-6">
                <div class="card border-success h-100" style="border-width: 0 0 0 2px;">
                    <div class="card-body">
                        <h5 class="text-success fw-bold">Modelo Cuantizado (Q4)</h5>
                        <div class="img-container">
                            <img src="Imagen2Cuantizacion.jpg" class="img-fluid" alt="Benchmark Q4">
                        </div>
                        <p class="explanation-text">
                            Bajo las mismas condiciones, el modelo cuantizado dispara su rendimiento hasta los 
                            <span class="badge bg-success metric-badge">174,6 tokens/seg</span>. 
                            Esto supone una mejora del <strong>83%</strong> en velocidad. Al ocupar menos memoria, la CPU puede procesar más datos en el mismo tiempo, reduciendo la latencia de respuesta para el usuario final.
                        </p>
                    </div>
                </div>
            </div>
        </div>

        <!-- SECCIÓN 3: CALIDAD -->
        <div class="row mt-5">
            <div class="col-12">
                <h3 class="mb-4 text-secondary border-start border-4 border-warning ps-3">3. Verificación de Calidad (LM Studio)</h3>
                <p class="mb-4 explanation-text">
                    Finalmente, cargamos ambos modelos en una interfaz gráfica (LM Studio) para evaluar cualitativamente las respuestas. 
                    Se busca detectar "alucinaciones" o incoherencias introducidas por la pérdida de precisión.
                </p>
            </div>

            <!-- Chat FP16 -->
            <div class="col-lg-12">
                <div class="card mb-4">
                    <div class="card-header bg-light">Respuesta del Modelo Original (FP16)</div>
                    <div class="card-body">
                        <div class="row align-items-center">
                            <div class="col-md-5">
                                <div class="img-container">
                                    <img src="Imagen5Cuantizacion.jpg" class="img-fluid" alt="Respuesta FP16">
                                </div>
                            </div>
                            <div class="col-md-7">
                                <p class="explanation-text">
                                    El modelo original genera una definición académica y estructurada sobre "Administración de Sistemas". 
                                    Utiliza un listado de 10 puntos clave, cubriendo desde el diseño de red hasta la seguridad. 
                                    La sintaxis es compleja y el vocabulario técnico es preciso.
                                </p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>

            <!-- Chat Q4 -->
            <div class="col-lg-12">
                <div class="card">
                    <div class="card-header bg-light">Respuesta del Modelo Cuantizado (Q4)</div>
                    <div class="card-body">
                        <div class="row align-items-center">
                            <div class="col-md-5">
                                <div class="img-container">
                                    <img src="Imagen6Cuantizacion.jpg" class="img-fluid" alt="Respuesta Q4">
                                </div>
                            </div>
                            <div class="col-md-7">
                                <p class="explanation-text">
                                    El modelo comprimido responde a la misma consulta. Sorprendentemente, <strong>mantiene la estructura de lista y los conceptos clave</strong>.
                                    Aunque la redacción varía ligeramente (como es natural en modelos generativos), no hay pérdida de información ni errores gramaticales.
                                    Esto confirma que la cuantización a 4 bits preserva el conocimiento semántico del modelo, haciéndolo ideal para producción.
                                </p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- CONCLUSIÓN FINAL -->
        <div class="row mt-5">
            <div class="col-12">
                <div class="alert alert-secondary p-4 rounded-3 border-0 bg-white shadow-sm">
                    <h4 class="alert-heading text-dark fw-bold mb-3">Conclusiones Finales</h4>
                    <p class="explanation-text mb-0">
                        La práctica ha demostrado empíricamente las ventajas de la cuantización en el despliegue de LLMs. 
                        Hemos transformado un modelo de 1,2 GB en uno de 469 MB, logrando que funcione casi al doble de velocidad (<strong>+83% tokens/s</strong>) 
                        sin sacrificar la calidad de las respuestas. Esta técnica es fundamental para ejecutar IAs potentes en hardware de consumo o servidores con recursos limitados.
                    </p>
                </div>
            </div>
        </div>

    </div>

    <!-- Footer -->
    <footer class="text-center">
        <div class="container">
            <p class="mb-1 fw-bold text-white">Práctica de Administración de Sistemas Informáticos en Red</p>
            <p class="small text-muted mb-0">Curso 2025/2026 - IES [Tu Instituto]</p>
        </div>
    </footer>

</body>
</html>
