<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Pr√°ctica de Destilaci√≥n de Modelos - ASIR</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            padding: 20px;
            line-height: 1.6;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 15px;
            box-shadow: 0 10px 40px rgba(0,0,0,0.2);
            overflow: hidden;
        }
        
        header {
            background: linear-gradient(135deg, #2c3e50 0%, #34495e 100%);
            color: white;
            padding: 30px;
            text-align: center;
        }
        
        header h1 {
            font-size: 2.5em;
            margin-bottom: 10px;
        }
        
        header p {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 40px;
        }
        
        .section {
            margin-bottom: 50px;
            background: #f8f9fa;
            border-radius: 10px;
            padding: 30px;
            box-shadow: 0 3px 10px rgba(0,0,0,0.1);
        }
        
        .section h2 {
            color: #2c3e50;
            font-size: 1.8em;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 3px solid #667eea;
        }
        
        .explanation {
            background: white;
            padding: 20px;
            border-radius: 8px;
            margin-bottom: 20px;
            border-left: 4px solid #667eea;
        }
        
        .explanation p {
            color: #34495e;
            font-size: 1.05em;
            margin-bottom: 10px;
        }
        
        .explanation ul {
            margin-left: 25px;
            color: #555;
        }
        
        .explanation li {
            margin-bottom: 8px;
        }
        
        .image-container {
            text-align: center;
            background: #2c3e50;
            padding: 20px;
            border-radius: 8px;
            margin-top: 20px;
        }
        
        .image-container img {
            max-width: 100%;
            height: auto;
            border-radius: 5px;
            box-shadow: 0 5px 15px rgba(0,0,0,0.3);
        }
        
        .badge {
            display: inline-block;
            background: #667eea;
            color: white;
            padding: 5px 15px;
            border-radius: 20px;
            font-size: 0.9em;
            margin-bottom: 15px;
        }
        
        footer {
            background: #2c3e50;
            color: white;
            text-align: center;
            padding: 20px;
            font-size: 0.9em;
        }
        
        .highlight {
            background: #fff3cd;
            padding: 2px 6px;
            border-radius: 3px;
            font-weight: bold;
        }
        
        .code-block {
            background: #1e1e1e;
            color: #d4d4d4;
            padding: 15px;
            border-radius: 5px;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
            overflow-x: auto;
            margin: 10px 0;
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>üß™ Pr√°ctica de Destilaci√≥n de Modelos</h1>
            <p>Knowledge Distillation con Qwen2.5 y Python</p>
        </header>
        
        <div class="content">
            <!-- Secci√≥n 1: Imagen 1 -->
            <div class="section">
                <span class="badge">Captura 1</span>
                <h2>Ejecuci√≥n del Script de Destilaci√≥n e Inicializaci√≥n</h2>
                
                <div class="explanation">
                    <p><strong>üìù Descripci√≥n:</strong></p>
                    <p>Esta captura muestra el inicio del proceso de destilaci√≥n ejecutando el script Python <code>medir_kl.py</code> desde la terminal. El sistema est√° en modo de entorno virtual (venv) y comienza el proceso de carga del modelo profesor.</p>
                    
                    <p><strong>üîç Detalles del proceso:</strong></p>
                    <ul>
                        <li><strong>Comando ejecutado:</strong> <code>python medir_kl.py</code></li>
                        <li><strong>Estado inicial:</strong> "MPS" (Metal Performance Shaders - aceleraci√≥n GPU en macOS)</li>
                        <li><strong>Fase 1 - Carga del Tokenizador:</strong>
                            <ul style="margin-top: 8px;">
                                <li>config.json: 100% (662/662 bytes, velocidad ~1920B/s)</li>
                                <li>vocab.json: 2.7MB (2.78/2.78 MB, velocidad 11.9MB/s)</li>
                                <li>merges.txt: 1.4MB (1.36/1.36 MB, velocidad 11.9MB/s)</li>
                                <li>tokenizer.json: 7.0MB (6.99/6.99 MB, velocidad 19.9MB/s)</li>
                            </ul>
                        </li>
                        <li><strong>Fase 2 - Carga del Modelo Profesor:</strong>
                            <ul style="margin-top: 8px;">
                                <li>Modelo: Qwen/Qwen2.5-0.5B-...</li>
                                <li>model.safetensors: 100% (291/291 archivos cargados)</li>
                                <li>Peso total: ~494MB cargados</li>
                                <li>Estado: "Materializing parameterized model..."</li>
                            </ul>
                        </li>
                        <li><strong>Fase 3 - Carga del Modelo Estudiante:</strong>
                            <ul style="margin-top: 8px;">
                                <li>Mismo proceso de carga</li>
                                <li>Modelo id√©ntico al profesor para comparaci√≥n</li>
                                <li>Barras de progreso muestran 643/661 y 291/291</li>
                            </ul>
                        </li>
                    </ul>
                    
                    <p><strong>‚ö° Resultado final:</strong></p>
                    <p>El sistema completa la carga de ambos modelos e inicia el c√°lculo: <span class="highlight">"Calculando KL para: 'La inteligencia artificial es'..."</span></p>
                    <p>Resultado obtenido: <span class="highlight">RESULTADO DIVERGENCIA KL: 1.706555</span></p>
                    
                    <p><strong>üí° Interpretaci√≥n:</strong></p>
                    <p>Este valor de divergencia KL (Kullback-Leibler) de 1.706555 mide la diferencia entre las distribuciones de probabilidad del modelo profesor y estudiante. Un valor mayor indica mayor diferencia entre ambos modelos. Durante el proceso de destilaci√≥n, este valor deber√≠a reducirse a medida que el estudiante aprende del profesor.</p>
                </div>
                
                <div class="image-container">
                    <img src="Imagen1Destilacion.png" alt="Imagen 1 - Ejecuci√≥n e inicializaci√≥n">
                </div>
            </div>
            
            <!-- Secci√≥n 2: Imagen 2 -->
            <div class="section">
                <span class="badge">Captura 2</span>
                <h2>Instalaci√≥n de Dependencias del Proyecto</h2>
                
                <div class="explanation">
                    <p><strong>üìù Descripci√≥n:</strong></p>
                    <p>Esta captura muestra la instalaci√≥n de las bibliotecas necesarias para ejecutar el proceso de destilaci√≥n. Se ejecuta el comando <code>pip install torch transformers accelerate</code> para instalar las dependencias principales del proyecto.</p>
                    
                    <p><strong>üì¶ Paquetes instalados:</strong></p>
                    <ul>
                        <li><strong>torch (PyTorch):</strong>
                            <ul style="margin-top: 8px;">
                                <li>Versi√≥n: 2.1.0 (optimizada para macOS 11.0+ ARM64)</li>
                                <li>Tama√±o descargado: 31 kB (usando cach√©)</li>
                                <li>Framework principal de deep learning para operaciones tensoriales y redes neuronales</li>
                            </ul>
                        </li>
                        <li><strong>transformers (Hugging Face):</strong>
                            <ul style="margin-top: 8px;">
                                <li>Versi√≥n: 5.1.0</li>
                                <li>Tama√±o descargado: 31 kB (usando cach√©)</li>
                                <li>Biblioteca para trabajar con modelos transformer (BERT, GPT, T5, Qwen, etc.)</li>
                                <li>Proporciona las clases para cargar y usar modelos pre-entrenados</li>
                            </ul>
                        </li>
                        <li><strong>accelerate:</strong>
                            <ul style="margin-top: 8px;">
                                <li>Versi√≥n: 1.2.0</li>
                                <li>Tama√±o descargado: 19 kB (usando cach√©)</li>
                                <li>Simplifica el entrenamiento en m√∫ltiples GPUs y entornos distribuidos</li>
                            </ul>
                        </li>
                    </ul>
                    
                    <p><strong>üîß Dependencias adicionales instaladas autom√°ticamente:</strong></p>
                    <ul>
                        <li><strong>filelock:</strong> 2.1 kB - Gesti√≥n de bloqueos de archivos</li>
                        <li><strong>typing_extensions:</strong> 4.15.0 (6.3 kB) - Extensiones de tipado para Python</li>
                        <li><strong>sympy:</strong> 1.13.1 (12 kB) - Biblioteca de matem√°ticas simb√≥licas</li>
                        <li><strong>networkx:</strong> 3.6.1 (6.8 kB) - An√°lisis y manipulaci√≥n de grafos</li>
                        <li><strong>jinja2:</strong> 3.1.7 (2.9 kB) - Motor de plantillas para Python</li>
                    </ul>
                    
                    <p><strong>‚úÖ Estado de la instalaci√≥n:</strong></p>
                    <p>Todos los paquetes se instalaron correctamente usando archivos en cach√© local, lo que aceler√≥ significativamente el proceso. No hubo errores durante la instalaci√≥n y el sistema est√° listo para ejecutar scripts de destilaci√≥n de modelos.</p>
                    
                    <p><strong>üí° Nota t√©cnica:</strong></p>
                    <p>La instalaci√≥n utiliza archivos en cach√© (<code>Using cached</code>), lo que indica que estos paquetes ya fueron descargados previamente en el sistema, optimizando el tiempo de configuraci√≥n del entorno.</p>
                </div>
                
                <div class="image-container">
                    <img src="Imagen2Destilacion.png" alt="Imagen 2 - Instalaci√≥n de dependencias">
                </div>
            </div>
            
            <!-- Secci√≥n 3: Imagen 3 -->
            <div class="section">
                <span class="badge">Captura 3</span>
                <h2>Comparaci√≥n de Predicciones: Profesor vs Estudiante</h2>
                
                <div class="explanation">
                    <p><strong>üìù Descripci√≥n:</strong></p>
                    <p>Esta captura muestra la comparaci√≥n de las predicciones Top-10 entre el modelo profesor y el modelo estudiante para el prompt <em>"La inteligencia artificial es"</em>. Los valores num√©ricos representan las probabilidades que cada modelo asigna a diferentes tokens (palabras) como continuaci√≥n del texto.</p>
                    
                    <p><strong>üìä PREDICCIONES TOP-10 DEL PROFESOR (Qwen2.5-0.5B):</strong></p>
                    <ul>
                        <li>1. "un" ‚Üí 0.300 (30.0%)</li>
                        <li>2. "una" ‚Üí 0.297 (29.7%)</li>
                        <li>3. "la" ‚Üí 0.075 (7.5%)</li>
                        <li>4. "una" ‚Üí 0.053 (5.3%)</li>
                        <li>5. "el" ‚Üí 0.033 (3.3%)</li>
                        <li>6. "capa" ‚Üí 0.008 (0.8%)</li>
                        <li>7. "i" ‚Üí 0.007 (0.7%)</li>
                        <li>8. "La" ‚Üí 0.006 (0.6%)</li>
                        <li>9. "ia" ‚Üí 0.005 (0.5%)</li>
                        <li>10. "ia" ‚Üí 0.007 (0.7%)</li>
                    </ul>
                    
                    <p><strong>üéì PREDICCIONES TOP-10 DEL ESTUDIANTE (modelo inicial):</strong></p>
                    <ul>
                        <li>1. "una" ‚Üí 0.082 (8.2%)</li>
                        <li>2. "la" ‚Üí 0.023 (2.3%)</li>
                        <li>3. "un" ‚Üí 0.017 (1.7%)</li>
                        <li>4. "el" ‚Üí 0.011 (1.1%)</li>
                        <li>5. "una" ‚Üí 0.007 (0.7%)</li>
                        <li>6. "capa" ‚Üí 0.006 (0.6%)</li>
                        <li>7. "La" ‚Üí 0.005 (0.5%)</li>
                        <li>8. "cama" ‚Üí 0.013 (1.3%)</li>
                        <li>9. "i" ‚Üí 0.005 (0.5%)</li>
                        <li>10. "ia" ‚Üí 0.007 (0.7%)</li>
                    </ul>
                    
                    <p><strong>üîç An√°lisis de las diferencias:</strong></p>
                    <ul>
                        <li><strong>Confianza del profesor:</strong> El modelo profesor tiene predicciones mucho m√°s confiadas, con los dos primeros tokens ("un" y "una") acumulando casi el 60% de la probabilidad total (0.300 + 0.297 = 0.597)</li>
                        <li><strong>Distribuci√≥n del estudiante:</strong> El modelo estudiante muestra una distribuci√≥n m√°s uniforme y menos confiada, con su predicci√≥n m√°s alta siendo solo 8.2% ("una")</li>
                        <li><strong>Orden diferente:</strong> Aunque ambos modelos predicen tokens similares, el orden de preferencia es distinto. El profesor prefiere "un" (masculino) primero, mientras que el estudiante prefiere "una" (femenino)</li>
                        <li><strong>Tokens inesperados:</strong> El estudiante predice "cama" con 1.3% de probabilidad, un token poco relevante que el profesor no prioriza tanto</li>
                    </ul>
                    
                    <p><strong>üìâ Resultado de Divergencia KL:</strong></p>
                    <p><span class="highlight">KL Divergencia: 1.706555</span></p>
                    
                    <p><strong>üí° Significado:</strong></p>
                    <p>La divergencia KL de 1.706555 cuantifica la diferencia entre estas dos distribuciones de probabilidad. Un valor alto indica que el estudiante todav√≠a est√° lejos de imitar las predicciones del profesor. El objetivo del entrenamiento de destilaci√≥n es minimizar esta divergencia, haciendo que el estudiante aprenda a predecir de manera similar al profesor sin necesitar el mismo tama√±o de modelo.</p>
                </div>
                
                <div class="image-container">
                    <img src="Imagen3Destilacion.png" alt="Imagen 3 - Comparaci√≥n de predicciones">
                </div>
            </div>
            
            <!-- Secci√≥n 4: Imagen 4 -->
            <div class="section">
                <span class="badge">Captura 4</span>
                <h2>Preparaci√≥n del Benchmark LAMBADA y Advertencias</h2>
                
                <div class="explanation">
                    <p><strong>üìù Descripci√≥n:</strong></p>
                    <p>Esta captura muestra la ejecuci√≥n del script <code>preguntas_dificiles_prof_para_conneccion.py</code>, que configura un benchmark de evaluaci√≥n usando el dataset LAMBADA. Durante la inicializaci√≥n, el sistema detecta flags de generaci√≥n incompatibles con la configuraci√≥n actual.</p>
                    
                    <p><strong>‚ö†Ô∏è ADVERTENCIAS DETECTADAS:</strong></p>
                    <div class="code-block">
The following generation flags are not valid and may be ignored: ['temperature'].
Set `TRANSFORMERS_VERBOSITY=info` for more details.
                    </div>
                    
                    <p><strong>üîß An√°lisis de las advertencias:</strong></p>
                    <ul>
                        <li><strong>Flag problem√°tico:</strong> <code>temperature</code></li>
                        <li><strong>Causa:</strong> El par√°metro 'temperature' no est√° soportado en el modo de generaci√≥n actual del modelo</li>
                        <li><strong>Impacto:</strong> El flag ser√° ignorado durante la generaci√≥n, lo que puede afectar la variabilidad de las respuestas</li>
                        <li><strong>Otros flags listados:</strong> 
                            <ul style="margin-top: 8px;">
                                <li>{'rep': 'repetition'} - Control de repetici√≥n de tokens</li>
                                <li>{'leng': 'length'} - Control de longitud de respuestas</li>
                                <li>{'freq': 'frequency'} - Control de frecuencia de tokens</li>
                            </ul>
                        </li>
                    </ul>
                    
                    <p><strong>üìä CONFIGURACI√ìN DEL BENCHMARK LAMBADA:</strong></p>
                    <p><strong>Nombre del archivo:</strong> <code>preguntas_dificiles_prof_para_conneccion.py</code></p>
                    <p>Este script est√° dise√±ado para:</p>
                    <ul>
                        <li><strong>Dataset LAMBADA:</strong> Language Modeling Broadened to Account for Discourse Aspects - un benchmark que eval√∫a la capacidad del modelo para predecir la √∫ltima palabra de un texto considerando el contexto largo</li>
                        <li><strong>Modo BENCHMARK ESTRICTO:</strong> Solo respuestas TOPK</li>
                        <li><strong>Modo de evaluaci√≥n:</strong> PROFESOR DOMINA - el modelo profesor gu√≠a la evaluaci√≥n</li>
                        <li><strong>Objetivo:</strong> Generar preguntas dif√≠ciles basadas en contextos largos para probar la comprensi√≥n del modelo estudiante</li>
                    </ul>
                    
                    <p><strong>üéØ BENCHMARK LAMBADA - Detalles:</strong></p>
                    <ul>
                        <li><strong>Prop√≥sito:</strong> Evaluar la capacidad de comprensi√≥n contextual a largo plazo</li>
                        <li><strong>Funcionamiento:</strong> Presenta textos donde la palabra final solo puede predecirse si se entiende todo el contexto previo</li>
                        <li><strong>Dificultad:</strong> Considerado uno de los benchmarks m√°s desafiantes para modelos de lenguaje</li>
                        <li><strong>Uso en destilaci√≥n:</strong> Permite identificar d√≥nde el estudiante necesita m√°s entrenamiento comparando sus predicciones con las del profesor</li>
                    </ul>
                    
                    <p><strong>üí° Recomendaciones:</strong></p>
                    <ul>
                        <li><strong>Para resolver la advertencia:</strong> Ejecutar <code>export TRANSFORMERS_VERBOSITY=info</code> antes del script para obtener m√°s detalles</li>
                        <li><strong>Alternativa:</strong> Modificar el c√≥digo para remover el par√°metro 'temperature' de las llamadas de generaci√≥n</li>
                        <li><strong>Validaci√≥n:</strong> Verificar que la ausencia del par√°metro 'temperature' no afecte los resultados del benchmark</li>
                    </ul>
                    
                    <p><strong>üîç Estado actual:</strong></p>
                    <p>El sistema est√° correctamente configurado para ejecutar el benchmark LAMBADA en modo "profesor domina" con validaci√≥n TOPK, aunque con advertencias sobre flags de generaci√≥n que ser√°n ignorados. El proceso puede continuar sin errores cr√≠ticos.</p>
                </div>
                
                <div class="image-container">
                    <img src="Imagen4Destilacion.png" alt="Imagen 4 - Benchmark LAMBADA">
                </div>
            </div>
            
        </div>
        
        <footer>
            <p><strong>Pr√°ctica de Destilaci√≥n de Modelos - ASIR</strong></p>
            <p>Febrero 2026 | Knowledge Distillation con Qwen2.5</p>
        </footer>
    </div>
</body>
</html>
